{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3185dc08",
   "metadata": {},
   "source": [
    "Web Scraping - Scraping the data from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe413e",
   "metadata": {},
   "source": [
    "BeautifulSoap is a class from bs4 python library (library used for web scraping), request is use to get the info via website link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87168240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a374f",
   "metadata": {},
   "source": [
    "'url' is the website link, headers is used to make the website feel that server is giving request to access the data so it is not interpreted as a bot request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a018d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ambitionbox.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.ambitionbox.com/list-of-companies?page=1'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"}\n",
    "r = requests.get(url, headers=headers, verify=False)\n",
    "print('STATUS', r.status_code)\n",
    "webpage = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a298de",
   "metadata": {},
   "source": [
    "It converts raw HTML into a structured object you can navigate like a tree.\n",
    "Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58661d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(webpage,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcee3b",
   "metadata": {},
   "source": [
    "It prints a formatted, indented version of the parsed HTML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5615f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dc128",
   "metadata": {},
   "source": [
    "To find all the h1 heading and print out the content using soup.find_all('h1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01dd0733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Companies in INDIA\n"
     ]
    }
   ],
   "source": [
    "clean = soup.find_all('h1')[0].get_text(separator=' ', strip=True).replace('\\xa0',' ')\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf611c",
   "metadata": {},
   "source": [
    "TO get all content with all h2 headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a1687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies in India\n",
      "TCS\n",
      "Accenture\n",
      "Wipro\n",
      "Cognizant\n",
      "Capgemini\n",
      "HDFC Bank\n",
      "Infosys\n",
      "ICICI Bank\n",
      "HCLTech\n",
      "Tech Mahindra\n",
      "Genpact\n",
      "Teleperformance\n",
      "Jio\n",
      "Axis Bank\n",
      "Concentrix Corporation\n",
      "Amazon\n",
      "iEnergizer\n",
      "Reliance Retail\n",
      "LTIMindtree\n",
      "IBM\n",
      "Popular Collections by Industries\n",
      "Popular Collections by Cities\n",
      "Popular Collections by Roles\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h2'):\n",
    "  print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34e79158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmbitionBox\n",
      "About Company\n",
      "Discover best places to work\n",
      "Compare & find best workplace\n",
      "Bring your workplace to life\n",
      "Highlight your company's perks\n",
      "Read reviews for 6L+ companies\n",
      "Rate your former or current company\n",
      "Discover salaries for 6L+ companies\n",
      "Calculate your take home salary\n",
      "Check your market value\n",
      "Help other jobseekers\n",
      "Check your gratuity amount\n",
      "Check how much of your HRA is tax-free\n",
      "Check your salary hike\n",
      "Read interviews for 40K+ companies\n",
      "Contribute your interview questions\n",
      "AmbitionBox Employee Choice Awards - 5th Edition\n",
      "AmbitionBox Employee Choice Awards - 4th Edition\n",
      "AmbitionBox Employee Choice Awards - 3rd Edition\n",
      "Popular\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "AmbitionBox Award Winner'25\n",
      "Side-by-side comparison to make informed career\n",
      "\t\t\t\t\t\t\t\t\t\tdecisions\n",
      "Trusted by over\n",
      "\t\t\t\t\t\t\t\t\t1.5 Crore\n",
      "\t\t\t\t\t\t\t\t\tjob seekers to find their right fit company\n",
      "Reviews\n",
      "Salaries\n",
      "Interviews\n",
      "Users\n",
      "About Company\n",
      "Made with ❤️ in India. Trademarks belong to their respective owners. All\n",
      "\t\t\t\t\trights reserved © 2026 Info Edge (India) Ltd.\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('p'):\n",
    "  print(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd5f09",
   "metadata": {},
   "source": [
    "To get the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6532fdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3\n",
      "3.7\n",
      "3.6\n",
      "3.6\n",
      "3.7\n",
      "3.8\n",
      "3.5\n",
      "4.0\n",
      "3.4\n",
      "3.4\n",
      "3.6\n",
      "3.8\n",
      "4.4\n",
      "3.6\n",
      "3.6\n",
      "3.9\n",
      "4.6\n",
      "3.9\n",
      "3.6\n",
      "3.9\n"
     ]
    }
   ],
   "source": [
    "ratings = soup.find_all('div', class_='rating_text')\n",
    "\n",
    "for r in ratings:\n",
    "    print(r.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da600f8",
   "metadata": {},
   "source": [
    "To find all the div which contain the info about the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07bef455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "company=soup.find_all('div',class_='companyCardWrapper')\n",
    "len(company)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786ca353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Name Rating               Company_type        Reviews  \\\n",
      "0                      TCS    3.3  IT Services & Consulting    1.1L Reviews   \n",
      "1                Accenture    3.7  IT Services & Consulting     72k Reviews   \n",
      "2                    Wipro    3.6  IT Services & Consulting   63.9k Reviews   \n",
      "3                Cognizant    3.6  IT Services & Consulting   60.3k Reviews   \n",
      "4                Capgemini    3.7  IT Services & Consulting   51.9k Reviews   \n",
      "5                HDFC Bank    3.8  IT Services & Consulting   51.1k Reviews   \n",
      "6                  Infosys    3.5  IT Services & Consulting   47.7k Reviews   \n",
      "7               ICICI Bank    4.0  IT Services & Consulting   45.2k Reviews   \n",
      "8                  HCLTech    3.4  IT Services & Consulting   44.9k Reviews   \n",
      "9            Tech Mahindra    3.4  IT Services & Consulting   42.6k Reviews   \n",
      "10                 Genpact    3.6  IT Services & Consulting   41.3k Reviews   \n",
      "11         Teleperformance    3.8  IT Services & Consulting   37.1k Reviews   \n",
      "12                     Jio    4.4  IT Services & Consulting   33.3k Reviews   \n",
      "13               Axis Bank    3.6  IT Services & Consulting   32.5k Reviews   \n",
      "14  Concentrix Corporation    3.6  IT Services & Consulting   31.6k Reviews   \n",
      "15                  Amazon    3.9  IT Services & Consulting   30.9k Reviews   \n",
      "16              iEnergizer    4.6  IT Services & Consulting   27.2k Reviews   \n",
      "17         Reliance Retail    3.9  IT Services & Consulting   27.1k Reviews   \n",
      "18             LTIMindtree    3.6  IT Services & Consulting     26k Reviews   \n",
      "19                     IBM    3.9  IT Services & Consulting   25.4k Reviews   \n",
      "\n",
      "          Salaries     Cities  \n",
      "0    9.9L Salaries  Bengaluru  \n",
      "1    6.5L Salaries  Bengaluru  \n",
      "2    4.8L Salaries  Hyderabad  \n",
      "3      6L Salaries  Hyderabad  \n",
      "4    4.8L Salaries  Bengaluru  \n",
      "5    1.5L Salaries     Mumbai  \n",
      "6    5.3L Salaries  Bengaluru  \n",
      "7    1.5L Salaries     Mumbai  \n",
      "8    3.8L Salaries    Chennai  \n",
      "9    2.8L Salaries  Hyderabad  \n",
      "10   2.3L Salaries  Hyderabad  \n",
      "11  96.1k Salaries     Mumbai  \n",
      "12  62.2k Salaries     Mumbai  \n",
      "13     1L Salaries     Mumbai  \n",
      "14   1.3L Salaries  Bengaluru  \n",
      "15   1.5L Salaries  Bengaluru  \n",
      "16  24.1k Salaries      Noida  \n",
      "17    76k Salaries     Mumbai  \n",
      "18   1.9L Salaries  Bengaluru  \n",
      "19   2.1L Salaries  Bengaluru  \n"
     ]
    }
   ],
   "source": [
    "name = []\n",
    "rating = []\n",
    "reviews = []\n",
    "salaries = []\n",
    "cities = []\n",
    "c_type = []\n",
    "\n",
    "for i in company:\n",
    "\n",
    "    name.append(i.find('h2').text.strip())\n",
    "    rating.append(i.find('div', class_='rating_text').text.strip())\n",
    "    \n",
    "    actions = i.find_all('a', class_='companyCardWrapper__ActionWrapper')\n",
    "    reviews.append(actions[0].text.strip())\n",
    "    salaries.append(actions[1].text.strip())\n",
    "    \n",
    "    info = i.find_all('span', class_='companyCardWrapper__interLinking')[0].text.strip()\n",
    "    \n",
    "    c_type, rest = info.split('|')\n",
    "    city = rest.split('+')[0].strip()\n",
    "    \n",
    "    cities.append(city)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': name,\n",
    "    'Rating': rating,\n",
    "    'Company_type': c_type,\n",
    "    'Reviews': reviews,\n",
    "    'Salaries': salaries,\n",
    "    'Cities': cities,\n",
    "})\n",
    "\n",
    "print(df)\n",
    "df.to_csv('company_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64f32cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 499: Status 200\n",
      "Page 499: Scraped 20 companies. Total: 20\n",
      "\n",
      "Total companies scraped: 20\n",
      "Data saved to company_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"}\n",
    "\n",
    "for j in range(499, 500):\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?page={j}'\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        print(f\"Page {j}: Status {r.status_code}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Page {j}: Error - {e}\")\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    company = soup.find_all('div', class_='companyCardWrapper')\n",
    "    \n",
    "    # Stop if no companies found (reached end of pages)\n",
    "    if not company:\n",
    "        print(f\"Page {j}: No companies found - stopping\")\n",
    "        break\n",
    "\n",
    "    name = []\n",
    "    rating = []\n",
    "    reviews = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    ctype_list = []\n",
    "\n",
    "    for i in company:\n",
    "        try:\n",
    "            company_name = i.find('h2').text.strip()\n",
    "            company_rating = i.find('div', class_='rating_text').text.strip()\n",
    "\n",
    "            actions = i.find_all('a', class_='companyCardWrapper__ActionWrapper')\n",
    "            company_reviews = actions[0].text.strip()\n",
    "            company_salaries = actions[1].text.strip()\n",
    "\n",
    "            info = i.find_all('span', class_='companyCardWrapper__interLinking')[0].text.strip()\n",
    "\n",
    "            ctype, sep, rest = info.partition('|')\n",
    "\n",
    "            if sep:\n",
    "                city = rest.split('+')[0].strip()\n",
    "            else:\n",
    "                city = None\n",
    "\n",
    "            # Only append if all data was extracted successfully\n",
    "            name.append(company_name)\n",
    "            rating.append(company_rating)\n",
    "            reviews.append(company_reviews)\n",
    "            salaries.append(company_salaries)\n",
    "            ctype_list.append(ctype.strip())\n",
    "            cities.append(city)\n",
    "        \n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Page {j}: Error parsing company - {e}\")\n",
    "            continue\n",
    "\n",
    "    d = pd.DataFrame({\n",
    "        'Name': name,\n",
    "        'Rating': rating,\n",
    "        'Company_type': ctype_list,\n",
    "        'Reviews': reviews,\n",
    "        'Salaries': salaries,\n",
    "        'Cities': cities,\n",
    "    })\n",
    "\n",
    "    df = pd.concat([df, d], ignore_index=True)\n",
    "    print(f\"Page {j}: Scraped {len(company)} companies. Total: {len(df)}\")\n",
    "    \n",
    "    # Be respectful to the server - add delay between requests\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nTotal companies scraped: {len(df)}\")\n",
    "df.to_csv('company_data.csv', index=False)\n",
    "print(\"Data saved to company_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26873d9",
   "metadata": {},
   "source": [
    "Making a dataset from website, finding the headings that are needed to be extracted and than inspecting the website and finding the data and converting it to a dataframe.\n",
    "Started of with 'url' (only one page for now) then giving 'headers' to avoid 403 error. Then used 'requests' to get the webpage and stored it in a variable 'webpage'. Using BeautifulSoup to parse the webpage and stored it in a variable 'soup'. Then used 'find_all' to find all the divs with class 'companyCardWrapper' and stored it in a variable 'company'. Then used a for loop to iterate through each company card and extract the required information and stored it in respective lists. Finally, created a dataframe using pandas and printed it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbba36",
   "metadata": {},
   "source": [
    "Basic fundamental for web scraping is to 'inspect' the website and hover down to the elements which you act upon and then use BeautifulSoup to extract that data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
