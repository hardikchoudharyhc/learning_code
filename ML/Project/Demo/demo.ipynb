{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3185dc08",
   "metadata": {},
   "source": [
    "Web Scraping - Scraping the data from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87168240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe413e",
   "metadata": {},
   "source": [
    "BeautifulSoap is a class from bs4 python library (library used for web scraping), request is use to get the info via website link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ambitionbox.com/list-of-companies?page=1'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"}\n",
    "r = requests.get(url, headers=headers, verify=False)\n",
    "print('STATUS', r.status_code)\n",
    "webpage = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a374f",
   "metadata": {},
   "source": [
    "'url' is the website link, headers is used to make the website feel that server is giving request to access the data so it is not interpreted as a bot request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58661d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(webpage,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a298de",
   "metadata": {},
   "source": [
    "It converts raw HTML into a structured object you can navigate like a tree.\n",
    "Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcee3b",
   "metadata": {},
   "source": [
    "It prints a formatted, indented version of the parsed HTML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = soup.find_all('h1')[0].get_text(separator=' ', strip=True).replace('\\xa0',' ')\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dc128",
   "metadata": {},
   "source": [
    "To find all the h1 heading and print out the content using soup.find_all('h1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('h2'):\n",
    "  print(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf611c",
   "metadata": {},
   "source": [
    "TO get all content with all h2 headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e79158",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('p'):\n",
    "  print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = soup.find_all('div', class_='rating_text')\n",
    "\n",
    "for r in ratings:\n",
    "    print(r.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd5f09",
   "metadata": {},
   "source": [
    "To get the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bef455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "company=soup.find_all('div',class_='companyCardWrapper')\n",
    "len(company)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da600f8",
   "metadata": {},
   "source": [
    "To find all the div which contain the info about the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "rating = []\n",
    "reviews = []\n",
    "salaries = []\n",
    "cities = []\n",
    "c_type = []\n",
    "\n",
    "for i in company:\n",
    "\n",
    "    name.append(i.find('h2').text.strip())\n",
    "    rating.append(i.find('div', class_='rating_text').text.strip())\n",
    "    \n",
    "    actions = i.find_all('a', class_='companyCardWrapper__ActionWrapper')\n",
    "    reviews.append(actions[0].text.strip())\n",
    "    salaries.append(actions[1].text.strip())\n",
    "    \n",
    "    info = i.find_all('span', class_='companyCardWrapper__interLinking')[0].text.strip()\n",
    "    \n",
    "    c_type, rest = info.split('|')\n",
    "    city = rest.split('+')[0].strip()\n",
    "    \n",
    "    cities.append(city)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': name,\n",
    "    'Rating': rating,\n",
    "    'Company_type': c_type,\n",
    "    'Reviews': reviews,\n",
    "    'Salaries': salaries,\n",
    "    'Cities': cities,\n",
    "})\n",
    "\n",
    "print(df)\n",
    "df.to_csv('company_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f32cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ambitionbox.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed page 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ambitionbox.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed page 276\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, d], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add delay between requests to avoid being blocked\u001b[39;00m\n\u001b[1;32m     72\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for j in range(1, 500):\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?page={j}'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"}\n",
    "\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    company = soup.find_all('div', class_='companyCardWrapper')\n",
    "\n",
    "    name = []\n",
    "    rating = []\n",
    "    reviews = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    ctype_list = []\n",
    "\n",
    "    for i in company:\n",
    "\n",
    "        name.append(i.find('h2').text.strip())\n",
    "        rating.append(i.find('div', class_='rating_text').text.strip())\n",
    "\n",
    "        actions = i.find_all('a', class_='companyCardWrapper__ActionWrapper')\n",
    "        reviews.append(actions[0].text.strip())\n",
    "        salaries.append(actions[1].text.strip())\n",
    "\n",
    "        info = i.find_all('span', class_='companyCardWrapper__interLinking')[0].text.strip()\n",
    "\n",
    "        ctype, sep, rest = info.partition('|')\n",
    "\n",
    "        if sep:\n",
    "            city = rest.split('+')[0].strip()\n",
    "        else:\n",
    "            city = None\n",
    "\n",
    "        ctype_list.append(ctype.strip())\n",
    "        cities.append(city)\n",
    "\n",
    "\n",
    "    d = pd.DataFrame({\n",
    "        'Name': name,\n",
    "        'Rating': rating,\n",
    "        'Company_type': ctype_list,\n",
    "        'Reviews': reviews,\n",
    "        'Salaries': salaries,\n",
    "        'Cities': cities,\n",
    "    })\n",
    "\n",
    "    df = pd.concat([df, d], ignore_index=True)\n",
    "\n",
    "df.to_csv('company_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26873d9",
   "metadata": {},
   "source": [
    "Making a dataset from website, finding the headings that are needed to be extracted and than inspecting the website and finding the data and converting it to a dataframe.\n",
    "Started of with 'url' (only one page for now) then giving 'headers' to avoid 403 error. Then used 'requests' to get the webpage and stored it in a variable 'webpage'. Using BeautifulSoup to parse the webpage and stored it in a variable 'soup'. Then used 'find_all' to find all the divs with class 'companyCardWrapper' and stored it in a variable 'company'. Then used a for loop to iterate through each company card and extract the required information and stored it in respective lists. Finally, created a dataframe using pandas and printed it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbba36",
   "metadata": {},
   "source": [
    "Basic fundamental for web scraping is to 'inspect' the website and hover down to the elements which you act upon and then use BeautifulSoup to extract that data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
